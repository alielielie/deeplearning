import hashlib
import os
import tarfile
import zipfile
import requests
import numpy as np
import pandas as pd
import torch
from torch import nn
from d2l import torch as d2l

DATA_HUB = dict()
DATA_URL = 'http://d2l-data.s3-accelerate.amazonaws.com/'

def download(name, cache_dir=os.path.join('.', 'data')):
    """ä¸‹è½½ä¸€ä¸ªDATA_HUBä¸­çš„æ–‡ä»¶ï¼Œè¿”å›æœ¬åœ°æ–‡ä»¶å"""
    assert name in DATA_HUB, f"{name} ä¸å­˜åœ¨äº {DATA_HUB}"
    url, sha1_hash = DATA_HUB[name]
    os.makedirs(cache_dir, exist_ok=True)
    fname = os.path.join(cache_dir, url.split('/')[-1])
    if os.path.exists(fname):
        sha1 = hashlib.sha1()
        with open(fname, 'rb') as f:
            while True:
                data = f.read(1048576)
                if not data:
                    break
                sha1.update(data)
        if sha1.hexdigest() == sha1_hash:
            return fname
    print(f'æ­£åœ¨ä»{url}ä¸‹è½½{fname}...')
    r = requests.get(url, stream=True, verify=True)
    with open(fname, 'wb') as f:
        f.write(r.content)
    return fname

def download_extract(name, folder=None):
    """ä¸‹è½½å¹¶è§£å‹zip/taræ–‡ä»¶"""
    fname = download(name)
    base_dir = os.path.dirname(fname)
    data_dir, ext = os.path.splitext(fname)
    if ext == '.zip':
        fp = zipfile.ZipFile(fname, 'r')
    elif ext in ('.tar', '.gz'):
        fp = tarfile.open(fname, 'r')
    else:
        assert False, 'åªæœ‰zip/taræ–‡ä»¶å¯ä»¥è¢«è§£å‹ç¼©'
    fp.extractall(base_dir)
    return os.path.join(base_dir, folder) if folder else data_dir

def download_all():
    """ä¸‹è½½DATA_HUBä¸­çš„æ‰€æœ‰æ–‡ä»¶"""
    for name in DATA_HUB:
        download(name)

DATA_HUB['kaggle_house_train'] = (
    DATA_URL + 'kaggle_house_pred_train.csv',
    '585e9cc93e70b39160e7921475f9bcd7d31219ce')

DATA_HUB['kaggle_house_test'] = (
    DATA_URL + 'kaggle_house_pred_test.csv',
    'fa19780a7b011d9b009e8bff8e99922a8ee2eb90')

train_data = pd.read_csv(download('kaggle_house_train'))
test_data = pd.read_csv(download('kaggle_house_test'))

print(train_data.shape)
print(test_data.shape)
# å‰å››ä¸ªå’Œæœ€åä¸¤ä¸ªç‰¹å¾ï¼Œä»¥åŠç›¸åº”æ ‡ç­¾
# print(train_data.iloc[0:4, [0, 1, 2, 3, -3, -2, -1]])

# åœ¨æ¯ä¸ªæ ·æœ¬ä¸­ï¼Œç¬¬ä¸€ä¸ªç‰¹å¾æ˜¯IDï¼Œ æˆ‘ä»¬å°†å…¶ä»æ•°æ®é›†ä¸­åˆ é™¤
all_features = pd.concat((train_data.iloc[:, 1:-1], test_data.iloc[:, 1:]))

# å°†æ‰€æœ‰ç¼ºå¤±çš„å€¼æ›¿æ¢ä¸ºç›¸åº”ç‰¹å¾çš„å¹³å‡å€¼ã€‚ é€šè¿‡å°†ç‰¹å¾é‡æ–°ç¼©æ”¾åˆ°é›¶å‡å€¼å’Œå•ä½æ–¹å·®æ¥æ ‡å‡†åŒ–æ•°æ®
numeric_features = all_features.dtypes[all_features.dtypes != 'object'].index
all_features[numeric_features] = all_features[numeric_features].apply(
    lambda x: (x - x.mean()) / (x.std()))
all_features[numeric_features] = all_features[numeric_features].fillna(0)

# å¤„ç†ç¦»æ•£å€¼ã€‚ æˆ‘ä»¬ç”¨ç‹¬çƒ­ç¼–ç æ›¿æ¢å®ƒä»¬
all_features = pd.get_dummies(all_features, dummy_na=True)
print(all_features.shape)

# ä»pandasæ ¼å¼ä¸­æå–NumPyæ ¼å¼ï¼Œå¹¶å°†å…¶è½¬æ¢ä¸ºå¼ é‡è¡¨ç¤º
n_train = train_data.shape[0]
train_features = torch.tensor(all_features[:n_train].values, dtype=torch.float32)
test_features = torch.tensor(all_features[n_train:].values, dtype=torch.float32)
train_labels = torch.tensor(
    train_data.SalePrice.values.reshape(-1, 1), dtype=torch.float32)

# è®­ç»ƒ
loss = nn.MSELoss()
in_features = train_features.shape[1]
print(in_features)

def get_net():
    net = nn.Sequential(
        nn.Linear(331, 512),
        nn.ReLU(),
        nn.Dropout(0.3),
        nn.Linear(512, 256),
        nn.ReLU(),
        nn.Dropout(0.1),
        nn.Linear(256, 128),
        nn.ReLU(),
        nn.Dropout(0.1),
        nn.Linear(128, 64),
        nn.ReLU(),
        nn.Dropout(0.1),
        nn.Linear(64, 1)
    )
    return net

# æˆ‘ä»¬æ›´å…³å¿ƒç›¸å¯¹è¯¯å·® ğ‘¦âˆ’ğ‘¦Ì‚ / ğ‘¦è§£å†³è¿™ä¸ªé—®é¢˜çš„ä¸€ç§æ–¹æ³•æ˜¯ç”¨ä»·æ ¼é¢„æµ‹çš„å¯¹æ•°æ¥è¡¡é‡å·®å¼‚
def log_rmse(net, features, labels):
    clipped_preds = torch.clamp(net(features), 1, float('inf'))
    rmse = torch.sqrt(loss(torch.log(clipped_preds), torch.log(labels)))
    return rmse.item()

# æˆ‘ä»¬çš„è®­ç»ƒå‡½æ•°å°†å€ŸåŠ©Adamä¼˜åŒ–å™¨
def train(net, train_features, train_labels, test_features, test_labels,
          num_epochs, learning_rate, weight_decay, batch_size):
    train_ls, test_ls = [], []
    train_iter = d2l.load_array((train_features, train_labels), batch_size)
    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)
    for epoch in range(num_epochs):
        for X, y in train_iter:
            optimizer.zero_grad()
            l = loss(net(X), y)
            l.backward()
            optimizer.step()
        train_ls.append(log_rmse(net, train_features, train_labels))
        if test_labels is not None:
            test_ls.append(log_rmse(net, test_features, test_labels))
    return train_ls, test_ls

# KæŠ˜äº¤å‰éªŒè¯
def get_k_fold_data(k, i, X, y):
    assert k > 1
    fold_size = X.shape[0] // k
    X_train, y_train = None, None
    for j in range(k):
        idx = slice(j * fold_size, (j + 1) * fold_size)
        X_part, y_part = X[idx, :], y[idx]
        if j == i:
            X_valid, y_valid = X_part, y_part
        elif X_train is None:
            X_train, y_train = X_part, y_part
        else:
            X_train = torch.cat([X_train, X_part], 0)
            y_train = torch.cat([y_train, y_part], 0)
    return X_train, y_train, X_valid, y_valid

# è¿”å›è®­ç»ƒå’ŒéªŒè¯è¯¯å·®çš„å¹³å‡å€¼
def k_fold(k, X_train, y_train, num_epochs, learning_rate, weight_decay,
           batch_size):
    train_l_sum, valid_l_sum = 0, 0
    for i in range(k):
        data = get_k_fold_data(k, i, X_train, y_train)
        net = get_net()
        train_ls, valid_ls = train(net, *data, num_epochs, learning_rate,
                                   weight_decay, batch_size)
        train_l_sum += train_ls[-1]
        valid_l_sum += valid_ls[-1]
        if i == 0:
            d2l.plot(list(range(1, num_epochs + 1)), [train_ls, valid_ls],
                     xlabel='epoch', ylabel='rmse', xlim=[1, num_epochs],
                     legend=['train', 'valid'], yscale='log')
        print(f'æŠ˜{i + 1}ï¼Œè®­ç»ƒlog rmse{float(train_ls[-1]):f}, '
              f'éªŒè¯log rmse{float(valid_ls[-1]):f}')
    return train_l_sum / k, valid_l_sum / k

# æ¨¡å‹é€‰æ‹©
k, num_epochs, lr, weight_decay, batch_size = 5, 50, 0.001, 1, 64
train_l, valid_l = k_fold(k, train_features, train_labels, num_epochs, lr,
                          weight_decay, batch_size)
print(f'{k}-æŠ˜éªŒè¯: å¹³å‡è®­ç»ƒlog rmse: {float(train_l):f}, '
      f'å¹³å‡éªŒè¯log rmse: {float(valid_l):f}')

# æäº¤Kaggleé¢„æµ‹
def train_and_pred(train_features, test_features, train_labels, test_data,
                   num_epochs, lr, weight_decay, batch_size):
    net = get_net()
    train_ls, _ = train(net, train_features, train_labels, None, None,
                        num_epochs, lr, weight_decay, batch_size)
    d2l.plot(np.arange(1, num_epochs + 1), [train_ls], xlabel='epoch',
             ylabel='log rmse', xlim=[1, num_epochs], yscale='log')
    print(f'è®­ç»ƒlog rmseï¼š{float(train_ls[-1]):f}')
    preds = net(test_features).detach().numpy()
    test_data['SalePrice'] = pd.Series(preds.reshape(1, -1)[0])
    submission = pd.concat([test_data['Id'], test_data['SalePrice']], axis=1)
    submission.to_csv('submission.csv', index=False)

# train_and_pred(train_features, test_features, train_labels, test_data,
#                num_epochs, lr, weight_decay, batch_size)

d2l.plt.show()